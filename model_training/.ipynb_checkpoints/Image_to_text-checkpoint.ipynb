{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd36b93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertConfig, ViTConfig, VisionEncoderDecoderConfig, VisionEncoderDecoderModel\n",
    "\n",
    "# Define configurations for the encoder (Vision Transformer) and decoder (BERT)\n",
    "encoder_config = \"google/vit-base-patch16-224-in21k\"\n",
    "decoder_config = \"bert-base-uncased\"\n",
    "\n",
    "# Create sub-configurations for the encoder and decoder\n",
    "encoder_sub_config = {\"config_name\": encoder_config}\n",
    "decoder_sub_config = {\"config_name\": decoder_config}\n",
    "\n",
    "# Create the main configuration for the Vision Encoder-Decoder model\n",
    "config = VisionEncoderDecoderConfig(encoder=encoder_sub_config, decoder=decoder_sub_config)\n",
    "\n",
    "# Instantiate the VisionEncoderDecoderModel with the configuration\n",
    "model = VisionEncoderDecoderModel(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "879890a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|█████████████████████████████████████████████████████████| 55/55 [00:00<00:00, 27498.71it/s]\n",
      "Resolving data files: 100%|█████████████████████████████████████████████████████████| 55/55 [00:00<00:00, 27390.97it/s]\n",
      "Downloading data files: 100%|████████████████████████████████████████████████████████| 55/55 [00:00<00:00, 9141.18it/s]\n",
      "Downloading data files: 0it [00:00, ?it/s]\n",
      "Extracting data files: 0it [00:00, ?it/s]\n",
      "Downloading data files: 100%|███████████████████████████████████████████████████████| 55/55 [00:00<00:00, 11020.24it/s]\n",
      "Downloading data files: 0it [00:00, ?it/s]\n",
      "Extracting data files: 0it [00:00, ?it/s]\n",
      "Generating train split: 55 examples [00:01, 38.48 examples/s]\n",
      "Generating validation split: 55 examples [00:00, 5189.10 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import list_datasets, load_dataset\n",
    "data=load_dataset('conceptual_captions')\n",
    "train_data=data[\"train\"]\n",
    "val_data=data[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a7fc76e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "class captiondatset(torch.utils.data.Dataset):\n",
    "    def __init__(self, datasets):\n",
    "        self.datasets = datasets\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "      image = Image.open(requests.get(self.datasets['image_url'][idx],stream=True).raw)\n",
    "      image_features = image_processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "      labels = tokenizer(self.datasets[\"caption\"][idx],return_tensors=\"pt\",\n",
    "                                          max_length=46,\n",
    "                                          pad_to_max_length=True,\n",
    "                                          return_token_type_ids=True,\n",
    "                                          truncation=True).input_ids\n",
    "      return {'pixel_values':image_features.squeeze(0),'labels':labels.squeeze(0)}\n",
    "datsets_train = captiondatset(train_data)\n",
    "dataset_val = captiondatset(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d31fc5d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer, TrainingArguments\n\u001b[0;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m model\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      6\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m      7\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m'\u001b[39m,          \u001b[38;5;66;03m# output directory\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,              \u001b[38;5;66;03m# total number of training epochs\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     logging_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     15\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model=model.to(device)\n",
    "model.train()\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',         \n",
    "    num_train_epochs=30,              \n",
    "    per_device_train_batch_size=16,  \n",
    "    per_device_eval_batch_size=64,   \n",
    "    warmup_steps=500,                \n",
    "    weight_decay=0.01,               \n",
    "    logging_dir='./logs',            \n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                        \n",
    "    args=training_args,                  \n",
    "    train_dataset=datsets_train,         \n",
    "    eval_dataset=dataset_val            \n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1629e1a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
